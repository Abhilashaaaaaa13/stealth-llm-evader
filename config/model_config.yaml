model:
  base: "mistralai/Mistral-7B-Instruct-v0.2"  
  lora_rank: 8
  lora_alpha: 32
  ensemble_models: ["mistralai/Mistral-7B-Instruct-v0.2", "microsoft/DialoGPT-large"]  # DialoGPT ko larger rakho if needed

evasion:
  burstiness_target: 0.9  # Std dev of sentence lengths
  perplexity_min: 20
  post_process_steps: ["vary_lengths", "paraphrase", "inject_idioms"]
  max_length: 2500  # Tokens for ~2000-3000 words

detectors:
  zerogpt_url: "https://api.zerogpt.com"  # Placeholder; get real API
  threshold: 0.20  # <20% AI score